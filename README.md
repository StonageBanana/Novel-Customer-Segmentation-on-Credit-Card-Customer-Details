# Novel-Customer-Segmentation-on-Credit-Card-Customer-Details

## ABSTRACT 
Customer segmentation is the process of dividing a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, income, behaviour, or location. The goal of customer segmentation is to identify high yield segments – that is, segments that are likely to be the most profitable or that have growth potential – and tailor marketing, products, or services to each group.
Customer segmentation is an important aspect of marketing, as it allows companies to effectively allocate their resources and tailor their offerings to the unique needs and preferences of different customer groups. This leads to a more efficient use of marketing resources, higher customer satisfaction, and ultimately, increased revenue.
There are many methods for performing customer segmentation, including demographic segmentation, psychographic segmentation, behavioural segmentation, and geographic segmentation. Each of these methods uses different criteria to divide the customer base into segments, and the choice of method will depend on the goals and resources of the company.
In recent years, advancements in data mining and machine learning have enabled more sophisticated and effective customer segmentation methods, allowing companies to make more informed and targeted marketing decisions. The results of customer segmentation can also be used in combination with other marketing data, such as customer lifetime value, to further optimize marketing efforts.


## Introduction
Customer segmentation is a crucial strategy for credit card companies as it helps them understand their customers better and allocate resources more efficiently. The problem statement for this project is to effectively segment the credit card customer base into different groups based on their characteristics and behaviors. This will enable credit card companies to make informed decisions about how to manage their customer portfolios, such as targeting certain segments with specific products or services and predicting the likelihood of customers to default on their credit card payments.
The objectives of this project are to identify the key characteristics and behaviors that differentiate credit card customers, segment the customer base into homogeneous groups based on these characteristics, and develop a model that can predict the likelihood of customers to default.
However, this project presents several challenges such as data quality and availability, privacy and security, complexity of customer behavior, balancing accuracy and interpretability, and integration with other systems. Ensuring the accuracy of the data collected is crucial, as it will impact the accuracy of the customer segmentation results. In addition, credit card companies need to comply with privacy and security regulations when collecting and using customer data. Understanding the complex behaviors of credit card customers is also a challenge, as they often exhibit unpredictable payment behaviors and use multiple credit cards. The results of the customer segmentation models need to be both accurate and interpretable by business users and should be integrated with other systems to maximize their value.
In conclusion, customer segmentation is a vital strategy for credit card companies to understand and manage their customer portfolios effectively. While the project presents several challenges, the benefits of better understanding and managing the customer base make it well worth the effort. 

## Dataset and Tool to be used.

The sample Dataset summarizes the usage behavior of about 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables.
The following is the structure of the data set:

Variable Name	Description	Sample Data
* cust_id:	Identification of Credit Card holder (Categorical)	C10001, C20002, ...
* balance:	Balance amount left in their account to make purchases	40.90, 3202.467, ...
* balance_frequency:	How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)	0.818, 0.909, ...
* purchases:	Amount of purchases made from account	95.4, 0, ...
* oneoff_purchases: Maximum purchase amount done in one-go	0, 773.17, ...
* installments_purchases:	Amount of purchase done in installment	95.4, 0, ...
* cash_advance: Cash in advance given by the user	0, 6442.945, ...
* purchases_frequency:	How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)	0.167, 0, ...
* oneoff_purchases_frequency:	How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)	0, 0.083, ...
* purchases_installments_frequency:	How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)	0.083, 0, ...
* cash_advance_frequency:	How frequently the cash in advance being paid	0.25, 0, ...
* cash_advance_trx:	Number of Transactions made with "Cash in Advanced" (discrete(integer))	0, 4, ...
* purchases_trx:	Number of purchase transactions made (discrete(integer))	2, 0, ...
* credit_limit:	Limit of Credit Card for user	1000, 7000, ...
* payments:	Amount of Payment done by user	0, 678.33, ...
* minimum_payments:	Minimum number of payments made by user	139.50, 1072.34, ...
* prc_full_payment:	Percent of full payment paid by user	0, 0.22, ...
* tenure:	Tenure of credit card service for user (discrete(integer))	12, 8, ...

## Algorithms / Techniques description

1.	K-Mean Clustering Analysis
K-means is a widely used unsupervised machine learning algorithm for clustering. It partitions a set of n data points into k clusters, where k is a user-specified number. The algorithm works by initializing k cluster centers and then assigning each data point to the nearest cluster center. The cluster centers are then updated based on the mean of the points in each cluster, and the assignment and update steps are repeated until convergence. The goal of the algorithm is to minimize the within-cluster sum of squared distances between the data points and their corresponding cluster center.
One advantage of K-means is that it is relatively fast and efficient, especially for large datasets. However, it has some limitations. For example, it is sensitive to the choice of initial cluster centers, and the results may depend on the order in which the data points are processed. Additionally, K-means is only appropriate for spherical, isotropic clusters and can struggle with more complex, non-linear structures.

2.	Minibatch K-Mean
Minibatch K-means is a variation of K-means that uses a smaller, randomly selected subset of the data, called a minibatch, to update the cluster centers. This can make the algorithm faster and more scalable for large datasets, since it reduces the number of calculations required at each iteration. Minibatch K-means can also be more robust to the choice of initial cluster centers, since the stochastic nature of the minibatch updates can help to avoid getting stuck in local minima.

3.	Mean Shift
Mean Shift is a non-parametric clustering algorithm that aims to find the modes or density peaks in a dataset. Unlike K-means, it does not require the number of clusters to be specified in advance. The algorithm works by initializing each data point as a cluster center and then shifting each cluster center towards areas of higher density. The process continues until convergence, at which point the cluster centers correspond to the modes of the data. The number of clusters is determined by the number of modes found in the data.
One advantage of Mean Shift is that it can find clusters of any shape, since it is based on density estimation rather than assumptions about the shape of the clusters. However, it can be computationally expensive, especially for large datasets, since it requires multiple passes over the data. Additionally, the choice of bandwidth, which determines the width of the density kernel, can have a significant impact on the results.

4.	Hierarchical Clustering
Hierarchical Clustering is a family of algorithms that build a hierarchy of clusters, where smaller clusters are merged into larger ones. There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative hierarchical clustering starts with each data point as a separate cluster and then merges the closest clusters into larger clusters. Divisive hierarchical clustering starts with all the data points in a single cluster and then splits the cluster into smaller ones. The result is a tree-like representation of the data, called a dendrogram, which can be used to visualize the relationships between the different clusters.
One advantage of Hierarchical Clustering is that it can reveal the natural hierarchical structure of the data, which can be useful for exploratory analysis. Additionally, it can be used to choose the number of clusters by cutting the dendrogram at a desired level. However, Hierarchical Clustering can be computationally expensive, especially for large datasets, and it can be difficult to interpret the results for more complex structures.
